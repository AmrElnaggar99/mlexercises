{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Reinforcement Learning Moon Lander (10 points)\n",
    "\n",
    "\n",
    "The goal of this exercise is to work with reinforcement learning models and get a basic understanding of the topic. We will first develop controlers for the simple cart pole model and later for the lunar lander.\n",
    "Neil Armstrong was the first to control a lunar lander in 1969. See a [video](https://youtu.be/xc1SzgGhMKc?t=520) about this masterpiece.\n",
    "Luckily we do not have to go to the moon, but can do our experiments in simulation based on the [Openai gym](https://gym.openai.com/) software.\n",
    "\n",
    "\n",
    "**Note**: openai gym is not well supported in anaconda. Please install gym in your conda environment using the following command:\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "pip install box2d box2d-kengz\n",
    "```\n",
    "\n",
    "**Note**: it can happend that the rendering window does not show up or close properly. In this case please check your environment and look for a solution and post it in the forum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress some warnings\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU support\n",
    "import tensorflow as tf\n",
    "print ( tf.__version__ ) \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Very basic RL example (1 points)\n",
    "\n",
    "Run this basic cart pole example and find out how it works and what the basic functions of gym are. Document the code with python comments. Find out what the observation and action values mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Result: documented code...\n",
    "#\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "cumulated_reward = 0\n",
    "for i in range(50):\n",
    "\n",
    "    env.render(mode='close')\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step( action )\n",
    "    cumulated_reward += reward\n",
    "    \n",
    "    print( '\\r', 'o:{} r:{} cr:{} d:{}   a:{}'.format(observation,reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.1)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Implement a basic on-off control strategy (1 points)\n",
    "\n",
    "Before we go into advanced control strategies, lets attempt to control the cart pole with a simple on-off control strategy. Reading the [documentation](https://github.com/openai/gym/wiki/CartPole-v0) of this gym we find that it has two actions (push cart left = 0 and push cart right = 1). So, one idea could be to just look at the pole's angle and push the cart left if the pole leans to the left and vice versa. Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "cumulated_reward = 0\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    env.render(mode='close')\n",
    "    \n",
    "    observation, reward, done, info = env.step( action )\n",
    "    cumulated_reward += reward\n",
    "\n",
    "    #\n",
    "    # Result: implement your control strategy here\n",
    "    #\n",
    "        \n",
    "    print( '\\r', 'a:{:.2f} p:{:.2f} r:{} cr:{} d:{}   a:{}'.format(observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        cululated_reward = 0\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.1)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: DQN Solution to cart pole balancing (2 point)\n",
    "\n",
    "Now lets build a first version based on advanced RL technique, the Deep Q-Network. Here a neural network is trained to estimate the best action for a state based on the Q-learning concept.\n",
    "\n",
    "The code is based on the work by Greg Surma and it can be found [here](https://github.com/gsurma/cartpole).\n",
    "\n",
    "Please go through the code and answer the questions in the comments of the code (marked by Task). \n",
    "\n",
    "**Note**: Place your answer as comment below the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "import gym\n",
    "\n",
    "prefix = 'results/04_dqn_'\n",
    "\n",
    "# hyperparameters from https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.0001\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 40\n",
    "EXPLORATION_MAX = 0.5\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "class DQNControl:\n",
    "\n",
    "    def __init__(self, observation_space, action_space,layout=[24,24],name='nona'):\n",
    "        \n",
    "        print ('building DQN model with observation space {} and action space {} layer {} name {}'.format(observation_space, action_space,layout,name) )\n",
    "        \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.name = name\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(layout[0], input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(layout[1], activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE,decay=LEARNING_RATE_DECAY ))\n",
    "\n",
    "        \n",
    "    def save(self):\n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        model_json = self.model.to_json()\n",
    "        with open( modelName , \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights( weightName )\n",
    "        print(\"saved model to disk as {} {}\".format(modelName,weightName))\n",
    "\n",
    "        \n",
    "    def load(self):    \n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        json_file = open(modelName, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(weightName)\n",
    "        print(\"loaded model from disk\")\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def action(self,state):\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        #\n",
    "        # Task: what is the purpose of this if statement\n",
    "        # Result: ....\n",
    "        #\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "\n",
    "        q_values = self.model.predict(state)\n",
    "        \n",
    "        #\n",
    "        # Task: what is the idea behind this step (to come from value to action)?\n",
    "        # Result: ....\n",
    "        #\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            \n",
    "            q_update = reward\n",
    "            if not done:\n",
    "                #\n",
    "                # Task: give an explanation for the formula of the update of the Q-value\n",
    "                # Result: ...\n",
    "                #\n",
    "                q_update = (reward + GAMMA * np.amax( self.model.predict(state_next)[0] ) )\n",
    "            \n",
    "            q_values = self.model.predict(state)\n",
    "            \n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def close_episode(self):\n",
    "        #\n",
    "        # Task: what is going on here?\n",
    "        # Result: ...\n",
    "        #\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def trainDQN(env,episodes=50,layout=[24,24], name='nona', termination_reward=None, termination_runs=None, termination_runs_reward=None ):\n",
    "    \n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    dqn_solver = DQNControl(observation_space, action_space,layout,name)\n",
    "    \n",
    "    history = []\n",
    "    run = 0\n",
    "    \n",
    "    accumulated_reward = 0\n",
    "    sliding_accumulated_reward = 0\n",
    "    \n",
    "    while run < episodes:\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            env.render(mode='close')\n",
    "            \n",
    "            action = dqn_solver.act(state)\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            accumulated_reward += reward\n",
    "            \n",
    "            if not (termination_runs is None) and step > termination_runs:\n",
    "                terminal = True\n",
    "                if not (termination_runs_reward is None):\n",
    "                    reward = termination_runs_reward\n",
    "            else:\n",
    "                if terminal and not (termination_reward is None):\n",
    "                    reward = termination_reward\n",
    "            \n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            state = state_next\n",
    "            \n",
    "            if terminal:\n",
    "                \n",
    "                sliding_accumulated_reward = sliding_accumulated_reward * 0.9 + accumulated_reward * 0.1\n",
    "                \n",
    "                print ( '\\r', 'episode: {}, exploration: {:.3f}, score: {} sliding score {}'.format(run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward), end='' )\n",
    "                \n",
    "                history.append([run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward,step])\n",
    "                \n",
    "                accumulated_reward = 0\n",
    "                break\n",
    "            \n",
    "            dqn_solver.experience_replay()\n",
    "        \n",
    "        \n",
    "        dqn_solver.close_episode()\n",
    "        \n",
    "        \n",
    "        run += 1\n",
    "\n",
    "    env.close()\n",
    "    return dqn_solver,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "control,history = trainDQN(env=env,episodes=60,layout=[24,24],name='cartdqn',termination_reward=-200,termination_runs=100,termination_runs_reward=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later\n",
    "control.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2].plot()\n",
    "df[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the DQN control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "control = DQNControl(observation_space, action_space)\n",
    "control.load()\n",
    "\n",
    "state = env.reset()\n",
    "cumulated_reward = 0\n",
    "\n",
    "for i in range(100):\n",
    "    env.render(mode='close')\n",
    "\n",
    "    # Result: implement your control strategy here\n",
    "    action = control.action( np.reshape(state, [1, observation_space]) )\n",
    "    observation, reward, done, _ = env.step( action )\n",
    "    \n",
    "    cumulated_reward += reward\n",
    "        \n",
    "    print( '\\r', 'a:{:.2f} p:{:.2f} r:{} cr:{} d:{}   a:{}'.format(observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        cumulated_reward = 0\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.05)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor critic model (2 points)\n",
    "\n",
    "Implement the actor critic model [1] for the cart pole gym. Lookup some tutorials and implement it in a similar structure as DQNControll. Compare the results.\n",
    "\n",
    "- [1] https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Result: actor critic solution for cart\n",
    "#\n",
    "class ACControl:\n",
    "    pass\n",
    "\n",
    "def trainACModel(episodes=500): \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accontrol, history = trainACModel(episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accontrol.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot scores\n",
    "#\n",
    "df[2].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander problem\n",
    "\n",
    "How we are looking into the lunar lander problem. We reuse the DQN controller from above with different parameters. Play with this problem and get an understanding of the rewards. Configuration is taken from [2]. A general discussion about this approach was published in [1].\n",
    "\n",
    "- [1] https://www.researchgate.net/publication/333145451_Deep_Q-Learning_on_Lunar_Lander_Game\n",
    "- [2] https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "control,history = trainDQN(env=env,episodes=150,layout=[64,32],name='lunar',termination_reward=None,termination_runs=150,termination_runs_reward=-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later\n",
    "control.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2].plot()\n",
    "df[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Implement an improved controler for the lunar lander (4 points)\n",
    "\n",
    "Search the internet for leadboards for lunar lander and try to implement one of the best solutions. Select your solution by simplicity and clarity of code. Comment the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Result: implementation of improved controler\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
